{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## INSTALLATIONS AND MOUNTING GOOGLE DRIVE"
      ],
      "metadata": {
        "id": "N3WnCEr4ZBsd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Install profanity check\n",
        "!pip install alt-profanity-check\n",
        "\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPl4icXEZDRn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03VNKJw4hBMd"
      },
      "source": [
        "# DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPROCESS DATASET\n",
        "\n",
        "Do only once, if required."
      ],
      "metadata": {
        "id": "2JoV8LM_RubD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k7mztu7C4hq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Preprocess DPO Training Data - Remove EOS tokens from jokes\n",
        "dataset_directory = '/content/drive/MyDrive/JOKEGPT_FOLDER/Datasets/DPO_Dataset'\n",
        "\n",
        "dataset_files = os.listdir(dataset_directory)\n",
        "\n",
        "for dataset_file in dataset_files:\n",
        "\n",
        "  # Load File to preprocess\n",
        "  dpo_dataset = pd.read_csv(os.path.join(\"/content/drive/MyDrive/JOKEGPT_FOLDER/Datasets/DPO_Dataset\", dataset_file))\n",
        "  print(f\"Loaded {dataset_file}\")\n",
        "\n",
        "  # Remove end of sequence token from each joke\n",
        "  for i in range(dpo_dataset.shape[0]):\n",
        "    if \"<eos>\" in dpo_dataset['losing_joke'].iloc[i]:\n",
        "      dpo_dataset['losing_joke'].iloc[i] = dpo_dataset['losing_joke'].iloc[i].split(\"<eos>\")[0]\n",
        "    if \"</s>\" in dpo_dataset['losing_joke'].iloc[i]:\n",
        "      dpo_dataset['losing_joke'].iloc[i] = dpo_dataset['losing_joke'].iloc[i].split(\"</s>\")[0]\n",
        "    if \"<|end_of_text|>\" in dpo_dataset['losing_joke'].iloc[i]:\n",
        "      dpo_dataset['losing_joke'].iloc[i] = dpo_dataset['losing_joke'].iloc[i].split(\"<|end_of_text|>\")[0]\n",
        "\n",
        "  print(\"Preprocessed jokes:\\n\", dpo_dataset['losing_joke'][:4])\n",
        "\n",
        "  # Save to new directory\n",
        "  output_dir = '/content/drive/MyDrive/JOKEGPT_FOLDER/Datasets/DPO_Dataset/After_preprocessing'\n",
        "\n",
        "  dpo_dataset.to_csv(os.path.join(output_dir, dataset_file), index=False)\n",
        "  print(f\"Saved {dataset_file} to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD PREPROCESSED DATASET"
      ],
      "metadata": {
        "id": "7xQyLFanS3Ip"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Preprocess DPO training data\n",
        "\n",
        "# Map the dataset to the desired format.\n",
        "def return_prompt_and_responses(samples):\n",
        "  return {\n",
        "    \"prompt\": samples[\"prompt\"],\n",
        "    \"chosen\": samples[\"winning_joke\"],\n",
        "    \"rejected\": samples[\"losing_joke\"],\n",
        " }\n",
        "\n",
        "# Choose the dataset to train\n",
        "dpo_dataset = pd.read_csv(\"/content/drive/MyDrive/JOKEGPT_FOLDER/Datasets/DPO_Dataset/After_preprocessing/results_dpo_llama3_04-55-37.csv\")\n",
        "column_names = ['winning_joke', 'losing_joke']\n",
        "\n",
        "dpo_dataset_hf = Dataset.from_pandas(dpo_dataset)\n",
        "\n",
        "dpo_dataset = dpo_dataset_hf.map(\n",
        "  return_prompt_and_responses,\n",
        "  batched=True,\n",
        "  remove_columns = column_names,\n",
        ")\n",
        "dpo_dataset, dpo_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIXqCeZuhGA6"
      },
      "source": [
        "# DPO TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E8-BWi7MzkRz"
      },
      "outputs": [],
      "source": [
        "# Patch DPO Trainer\n",
        "from unsloth import PatchDPOTrainer\n",
        "PatchDPOTrainer()\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "from trl import DPOTrainer\n",
        "from google.colab import drive\n",
        "from numba import cuda\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Choose model to train\n",
        "model_path = \"/content/drive/MyDrive/JOKEGPT_FOLDER/Models/llama3\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_path,\n",
        "        max_seq_length = 2048,\n",
        "        dtype = None,\n",
        "        load_in_4bit = True\n",
        "    )\n",
        "\n",
        "# Set hyper-parameters for DPO Training\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model = model,\n",
        "    ref_model = None,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 5,\n",
        "        gradient_accumulation_steps = 10,\n",
        "        warmup_ratio = 0.1,\n",
        "        num_train_epochs = 2,\n",
        "        learning_rate = 2e-7,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        # weight_decay = 1e-3,\n",
        "        # max_grad_norm = 3,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 42,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        "    beta = 0.1,\n",
        "    train_dataset = dpo_dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    max_length = 1024,\n",
        "    max_prompt_length = 512,\n",
        ")\n",
        "\n",
        "# Training\n",
        "dpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaMHjz4VhLqw"
      },
      "source": [
        "**SAVE** **MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3MMJuAygtcM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "save_to_dir = \"/content/drive/MyDrive/JOKEGPT_FOLDER/Models/After_DPO\"\n",
        "\n",
        "# Add timestamp to the directory path\n",
        "# timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "save_to_dir = os.path.join(save_to_dir, f\"dpo_llama3\")\n",
        "\n",
        "# Save the model and tokenizer with the timestamp included\n",
        "model.save_pretrained(save_to_dir)\n",
        "tokenizer.save_pretrained(save_to_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBlEsfQXg7Gc"
      },
      "source": [
        "# INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-_-4YDVYyEu"
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Choose model to perform inference\n",
        "model_path = \"/content/drive/MyDrive/JOKEGPT_FOLDER/Models/After_DPO/dpo_llama2\"\n",
        "\n",
        "dpo_fine_model, dpo_fine_tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_path,\n",
        "        max_seq_length = 2048,\n",
        "        dtype = None,\n",
        "        load_in_4bit = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21PUEq-qhRVJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from profanity_check import predict, predict_prob\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = pd.read_csv(\"/content/drive/MyDrive/JOKEGPT_FOLDER/Datasets/jokes_test_dataset.csv\", header=None)\n",
        "\n",
        "test_dataset = test_dataset.rename(columns={0: 'joke', 1: 'prompt'})\n",
        "prompts = [element for element in test_dataset['prompt'].values]\n",
        "\n",
        "FastLanguageModel.for_inference(dpo_fine_model)\n",
        "joke_prompt =\"\"\"### Prompt:{} ### Joke:{}\"\"\"\n",
        "\n",
        "# Generate results for each prompt\n",
        "dpo_fineTunedJokes = []\n",
        "for prompt in prompts:\n",
        "  inputs = dpo_fine_tokenizer(\n",
        "          [\n",
        "              joke_prompt.format(\n",
        "                  prompt,\n",
        "                  \"\",\n",
        "              )\n",
        "          ], return_tensors = \"pt\").to(\"cuda\")\n",
        "  outputs = dpo_fine_model.generate(**inputs, max_new_tokens = 200, use_cache = True, do_sample=True)\n",
        "\n",
        "  # Regenerate results if the joke is profane for upto 3 attempts\n",
        "  appropriate_answer = False\n",
        "  attempt_counter = 0\n",
        "  while not appropriate_answer:\n",
        "    # Generate output\n",
        "    outputs = dpo_fine_model.generate(**inputs, max_new_tokens=64, use_cache=True, do_sample=True)\n",
        "    # Get response\n",
        "    res = dpo_fine_tokenizer.batch_decode(outputs)\n",
        "    if res is None or len(res) == 0:\n",
        "        print(\"res is empty: trying again\")\n",
        "        continue\n",
        "\n",
        "    # Check that answer is appropriate\n",
        "    profane_score = predict_prob(res)\n",
        "    if profane_score < 0.5 or attempt_counter >= 2:\n",
        "        # If so, add to list and exit loop; otherwise, generate another joke\n",
        "        # Extract joke only from text\n",
        "        res = res[0].split(\"### Joke:\")[1]\n",
        "\n",
        "        dpo_fineTunedJokes.append(res)\n",
        "        appropriate_answer = True\n",
        "    else:\n",
        "        print(f\"Too profane! Trying again.\\nScore: {profane_score}, Response: {res}\")\n",
        "        attempt_counter += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qthcx28Cd_Mi"
      },
      "outputs": [],
      "source": [
        "# Remove EOS token from outputs\n",
        "test_dataset['dpo_joke'] = [s.replace('</s>', '') for s in dpo_fineTunedJokes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJs5cORTbPvJ"
      },
      "outputs": [],
      "source": [
        "# Save to result folder\n",
        "test_dataset.to_csv('/content/drive/MyDrive/JOKEGPT_FOLDER/Results/SFT_DPO/llama3.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}